{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fd2ade8",
   "metadata": {},
   "source": [
    "# ğŸ”¬ Smart Lens v2 â€” Fine-Tune Existing Model with More Data\n",
    "\n",
    "**Strategy: Transfer Learning from your trained `best.pt`**\n",
    "\n",
    "This notebook does NOT start from scratch. It loads your already-trained model and fine-tunes it with additional open-source datasets merged with your existing data.\n",
    "\n",
    "### Why this works:\n",
    "- Your `best.pt` already has learned feature representations for Fighting, Fire, Gun, Knife\n",
    "- Fine-tuning preserves those features while improving from new data\n",
    "- Much faster than training from scratch (fewer epochs needed)\n",
    "- Results in better accuracy because the model starts from a good baseline\n",
    "\n",
    "### Pipeline:\n",
    "1. âœ… Mount Drive & upload your `best.pt`\n",
    "2. âœ… Download your existing dataset (Roboflow)\n",
    "3. âœ… Download additional open-source datasets for each class\n",
    "4. âœ… Merge & deduplicate all datasets into unified YOLOv8 format\n",
    "5. âœ… Fine-tune from `best.pt` with merged dataset\n",
    "6. âœ… Evaluate & compare with v1\n",
    "7. âœ… Export & download improved model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adc521e",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ› ï¸ Section 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d48670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1A: Install dependencies\n",
    "# ============================================================\n",
    "!pip install -q ultralytics roboflow opencv-python-headless\n",
    "\n",
    "import os, shutil, glob, yaml, json, random, time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d3f43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1B: Mount Google Drive (to upload/download model files)\n",
    "# ============================================================\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create project folder in Drive\n",
    "DRIVE_DIR = '/content/drive/MyDrive/Smart-Lens-FYP'\n",
    "os.makedirs(DRIVE_DIR, exist_ok=True)\n",
    "os.makedirs(f'{DRIVE_DIR}/models', exist_ok=True)\n",
    "print(f'Drive directory: {DRIVE_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4819e335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1C: Upload your existing best.pt model\n",
    "# ============================================================\n",
    "# Option 1: Upload from local machine\n",
    "from google.colab import files\n",
    "\n",
    "MODEL_PATH = '/content/best.pt'\n",
    "\n",
    "# Check if already in Drive\n",
    "drive_model = f'{DRIVE_DIR}/models/best_v1.pt'\n",
    "if os.path.exists(drive_model):\n",
    "    shutil.copy(drive_model, MODEL_PATH)\n",
    "    print(f'Loaded model from Drive: {drive_model}')\n",
    "elif not os.path.exists(MODEL_PATH):\n",
    "    print('Upload your best.pt file:')\n",
    "    uploaded = files.upload()\n",
    "    for name in uploaded:\n",
    "        shutil.move(name, MODEL_PATH)\n",
    "    # Save backup to Drive\n",
    "    shutil.copy(MODEL_PATH, drive_model)\n",
    "    print(f'Saved backup to Drive: {drive_model}')\n",
    "\n",
    "print(f'Model ready: {MODEL_PATH} ({os.path.getsize(MODEL_PATH)/1024/1024:.1f} MB)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399a1683",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“Š Section 2: Download & Merge Datasets\n",
    "\n",
    "### Additional Open-Source Datasets (Roboflow Universe)\n",
    "\n",
    "We'll merge your existing data with these high-quality labeled datasets:\n",
    "\n",
    "| Class | Additional Dataset | Source | Why |\n",
    "|-------|-------------------|--------|-----|\n",
    "| **Gun** | Handgun Detection | Roboflow Universe | More gun angles, CCTV-style |\n",
    "| **Gun** | Weapons Detection | Roboflow Universe | Various firearms |\n",
    "| **Knife** | Knife Detection | Roboflow Universe | More knife variations |\n",
    "| **Fire** | Fire & Smoke | Roboflow Universe | Indoor/outdoor fire scenes |\n",
    "| **Fighting** | Violence Detection | Roboflow Universe | Surveillance-style fighting |\n",
    "\n",
    "All datasets are remapped to match your class IDs:\n",
    "- `0` = Fighting\n",
    "- `1` = Fire\n",
    "- `2` = Gun\n",
    "- `3` = Knife"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aca350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2A: Download your ORIGINAL dataset\n",
    "# ============================================================\n",
    "from roboflow import Roboflow\n",
    "\n",
    "rf = Roboflow(api_key=\"7QsEv54uizzlrvPZ972Z\")\n",
    "project = rf.workspace(\"fpy\").project(\"smart-survellaince-lens-2\")\n",
    "version = project.version(1)\n",
    "original_ds = version.download(\"yolov8\", location='/content/datasets/original')\n",
    "print(f'Original dataset downloaded to: /content/datasets/original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8519df22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2B: Download ADDITIONAL open-source datasets\n",
    "# ============================================================\n",
    "# These are popular, well-labeled datasets from Roboflow Universe.\n",
    "# Each has its own class mapping that we'll remap to match yours.\n",
    "#\n",
    "# NOTE: If any dataset API fails (quota/removed), the script\n",
    "# will skip it and continue with whatever is available.\n",
    "# You can also add your own datasets here!\n",
    "\n",
    "ADDITIONAL_DATASETS = [\n",
    "    # â”€â”€ Gun datasets â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    {\n",
    "        \"name\": \"Handgun Detection (OD)\",\n",
    "        \"workspace\": \"object-detections-jgfbs\",\n",
    "        \"project\": \"handgun-detection-yjebf\",\n",
    "        \"version\": 2,\n",
    "        \"api_key\": \"7QsEv54uizzlrvPZ972Z\",  # Use your key or the public one\n",
    "        \"class_map\": {\"handgun\": 2, \"Handgun\": 2, \"gun\": 2, \"Gun\": 2, \"pistol\": 2, \"Pistol\": 2},\n",
    "        \"target_classes\": [2],  # Gun\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Pistol Detection\",\n",
    "        \"workspace\": \"dss-cctv\",\n",
    "        \"project\": \"pistol-detection-bvaxq\",\n",
    "        \"version\": 1,\n",
    "        \"api_key\": \"7QsEv54uizzlrvPZ972Z\",\n",
    "        \"class_map\": {\"pistol\": 2, \"Pistol\": 2, \"gun\": 2, \"Gun\": 2},\n",
    "        \"target_classes\": [2],\n",
    "    },\n",
    "    # â”€â”€ Knife datasets â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    {\n",
    "        \"name\": \"Knife Detection\",\n",
    "        \"workspace\": \"knife-5wkrr\",\n",
    "        \"project\": \"knife-detection-l8s6h\",\n",
    "        \"version\": 1,\n",
    "        \"api_key\": \"7QsEv54uizzlrvPZ972Z\",\n",
    "        \"class_map\": {\"knife\": 3, \"Knife\": 3},\n",
    "        \"target_classes\": [3],\n",
    "    },\n",
    "    # â”€â”€ Fire datasets â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    {\n",
    "        \"name\": \"Fire and Smoke Detection\",\n",
    "        \"workspace\": \"dsti\",\n",
    "        \"project\": \"fire-and-smoke-dsti\",\n",
    "        \"version\": 1,\n",
    "        \"api_key\": \"7QsEv54uizzlrvPZ972Z\",\n",
    "        \"class_map\": {\"fire\": 1, \"Fire\": 1, \"smoke\": 1, \"Smoke\": 1},\n",
    "        \"target_classes\": [1],\n",
    "    },\n",
    "]\n",
    "\n",
    "# Download each dataset\n",
    "downloaded_paths = []\n",
    "for i, ds_info in enumerate(ADDITIONAL_DATASETS):\n",
    "    print(f'\\n[{i+1}/{len(ADDITIONAL_DATASETS)}] Downloading: {ds_info[\"name\"]}...')\n",
    "    try:\n",
    "        rf2 = Roboflow(api_key=ds_info[\"api_key\"])\n",
    "        proj = rf2.workspace(ds_info[\"workspace\"]).project(ds_info[\"project\"])\n",
    "        ver = proj.version(ds_info[\"version\"])\n",
    "        dl_path = f'/content/datasets/extra_{i}'\n",
    "        ver.download(\"yolov8\", location=dl_path)\n",
    "        ds_info[\"local_path\"] = dl_path\n",
    "        downloaded_paths.append(ds_info)\n",
    "        print(f'  âœ… Downloaded to {dl_path}')\n",
    "    except Exception as e:\n",
    "        print(f'  âŒ Failed: {e}')\n",
    "        print(f'  Skipping this dataset, continuing...')\n",
    "\n",
    "print(f'\\nâœ… {len(downloaded_paths)}/{len(ADDITIONAL_DATASETS)} additional datasets downloaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd595eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2C: MERGE all datasets into one unified dataset\n",
    "# ============================================================\n",
    "# This is the critical step: remap class IDs, copy images+labels,\n",
    "# handle duplicates, and create a unified data.yaml\n",
    "\n",
    "MERGED_DIR = '/content/merged_dataset'\n",
    "os.makedirs(f'{MERGED_DIR}/train/images', exist_ok=True)\n",
    "os.makedirs(f'{MERGED_DIR}/train/labels', exist_ok=True)\n",
    "os.makedirs(f'{MERGED_DIR}/valid/images', exist_ok=True)\n",
    "os.makedirs(f'{MERGED_DIR}/valid/labels', exist_ok=True)\n",
    "os.makedirs(f'{MERGED_DIR}/test/images', exist_ok=True)\n",
    "os.makedirs(f'{MERGED_DIR}/test/labels', exist_ok=True)\n",
    "\n",
    "# Our target class mapping\n",
    "TARGET_CLASSES = {0: 'Fighting', 1: 'Fire', 2: 'Gun', 3: 'Knife'}\n",
    "stats = {'total_images': 0, 'total_labels': 0, 'per_class': Counter(), 'per_source': Counter()}\n",
    "\n",
    "def copy_dataset(src_dir, prefix, class_remap=None, source_data_yaml=None):\n",
    "    \"\"\"Copy images+labels from a YOLOv8 dataset to merged dir.\n",
    "    \n",
    "    class_remap: dict mapping source_class_name -> target_class_id\n",
    "                 If None, assume same class IDs as target.\n",
    "    source_data_yaml: path to source data.yaml to read class names\n",
    "    \"\"\"\n",
    "    # Read source class names if we need to remap\n",
    "    src_class_names = {}\n",
    "    if source_data_yaml and os.path.exists(source_data_yaml):\n",
    "        with open(source_data_yaml) as f:\n",
    "            src_yaml = yaml.safe_load(f)\n",
    "            names = src_yaml.get('names', [])\n",
    "            if isinstance(names, list):\n",
    "                src_class_names = {i: n for i, n in enumerate(names)}\n",
    "            elif isinstance(names, dict):\n",
    "                src_class_names = names\n",
    "    \n",
    "    copied = 0\n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        img_dir = os.path.join(src_dir, split, 'images')\n",
    "        lbl_dir = os.path.join(src_dir, split, 'labels')\n",
    "        \n",
    "        if not os.path.isdir(img_dir):\n",
    "            continue\n",
    "        \n",
    "        # Map all extra valid/test to train (we'll re-split later)\n",
    "        target_split = split\n",
    "        \n",
    "        for img_file in glob.glob(os.path.join(img_dir, '*')):\n",
    "            img_name = os.path.basename(img_file)\n",
    "            stem = Path(img_name).stem\n",
    "            ext = Path(img_name).suffix\n",
    "            lbl_file = os.path.join(lbl_dir, f'{stem}.txt')\n",
    "            \n",
    "            # New unique name with prefix\n",
    "            new_name = f'{prefix}_{stem}'\n",
    "            new_img = os.path.join(MERGED_DIR, target_split, 'images', f'{new_name}{ext}')\n",
    "            new_lbl = os.path.join(MERGED_DIR, target_split, 'labels', f'{new_name}.txt')\n",
    "            \n",
    "            # Skip if no label file\n",
    "            if not os.path.exists(lbl_file):\n",
    "                continue\n",
    "            \n",
    "            # Remap labels if needed\n",
    "            if class_remap and src_class_names:\n",
    "                remapped_lines = []\n",
    "                with open(lbl_file) as f:\n",
    "                    for line in f:\n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) < 5:\n",
    "                            continue\n",
    "                        src_cls_id = int(parts[0])\n",
    "                        src_cls_name = src_class_names.get(src_cls_id, '')\n",
    "                        \n",
    "                        # Try to find target class ID\n",
    "                        target_cls = None\n",
    "                        for src_key, tgt_id in class_remap.items():\n",
    "                            if src_key.lower() == src_cls_name.lower():\n",
    "                                target_cls = tgt_id\n",
    "                                break\n",
    "                        # Also try direct ID mapping\n",
    "                        if target_cls is None and src_cls_id in class_remap.values():\n",
    "                            target_cls = src_cls_id\n",
    "                        \n",
    "                        if target_cls is not None:\n",
    "                            parts[0] = str(target_cls)\n",
    "                            remapped_lines.append(' '.join(parts))\n",
    "                            stats['per_class'][TARGET_CLASSES[target_cls]] += 1\n",
    "                \n",
    "                if not remapped_lines:\n",
    "                    continue  # No valid labels after remap\n",
    "                \n",
    "                shutil.copy2(img_file, new_img)\n",
    "                with open(new_lbl, 'w') as f:\n",
    "                    f.write('\\n'.join(remapped_lines) + '\\n')\n",
    "            else:\n",
    "                # Direct copy (same class structure)\n",
    "                shutil.copy2(img_file, new_img)\n",
    "                shutil.copy2(lbl_file, new_lbl)\n",
    "                # Count per class\n",
    "                with open(lbl_file) as f:\n",
    "                    for line in f:\n",
    "                        parts = line.strip().split()\n",
    "                        if parts:\n",
    "                            cls_id = int(parts[0])\n",
    "                            cls_name = TARGET_CLASSES.get(cls_id, f'Unknown_{cls_id}')\n",
    "                            stats['per_class'][cls_name] += 1\n",
    "            \n",
    "            copied += 1\n",
    "    \n",
    "    return copied\n",
    "\n",
    "\n",
    "# â”€â”€ Step 1: Copy original dataset â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print('Copying original dataset...')\n",
    "n = copy_dataset('/content/datasets/original', prefix='orig')\n",
    "stats['per_source']['Original'] = n\n",
    "stats['total_images'] += n\n",
    "print(f'  âœ… {n} images from original dataset')\n",
    "\n",
    "# â”€â”€ Step 2: Copy additional datasets with class remapping â”€â”€â”€â”€\n",
    "for ds_info in downloaded_paths:\n",
    "    name = ds_info['name']\n",
    "    path = ds_info['local_path']\n",
    "    remap = ds_info['class_map']\n",
    "    prefix = name.replace(' ', '_').lower()[:15]\n",
    "    \n",
    "    # Find data.yaml in downloaded dataset\n",
    "    data_yaml = os.path.join(path, 'data.yaml')\n",
    "    \n",
    "    print(f'\\nMerging: {name}...')\n",
    "    if os.path.exists(data_yaml):\n",
    "        with open(data_yaml) as f:\n",
    "            src_yaml = yaml.safe_load(f)\n",
    "            print(f'  Source classes: {src_yaml.get(\"names\", \"?\")}')\n",
    "    \n",
    "    n = copy_dataset(path, prefix=prefix, class_remap=remap, source_data_yaml=data_yaml)\n",
    "    stats['per_source'][name] = n\n",
    "    stats['total_images'] += n\n",
    "    print(f'  âœ… {n} images merged (remapped to Smart Lens classes)')\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'MERGE COMPLETE')\n",
    "print(f'{\"=\"*60}')\n",
    "print(f'Total images: {stats[\"total_images\"]}')\n",
    "print(f'\\nPer source:')\n",
    "for src, count in stats['per_source'].items():\n",
    "    print(f'  {src}: {count}')\n",
    "print(f'\\nPer class (annotations):')\n",
    "for cls, count in stats['per_class'].most_common():\n",
    "    print(f'  {cls}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae03ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2D: Re-split merged dataset (80/15/5) and create data.yaml\n",
    "# ============================================================\n",
    "# Some extra datasets may have put everything in train/.\n",
    "# Let's do a clean 80/15/5 split of the full merged set.\n",
    "\n",
    "all_train_imgs = glob.glob(f'{MERGED_DIR}/train/images/*')\n",
    "all_valid_imgs = glob.glob(f'{MERGED_DIR}/valid/images/*')\n",
    "all_test_imgs = glob.glob(f'{MERGED_DIR}/test/images/*')\n",
    "\n",
    "print(f'Before re-split: train={len(all_train_imgs)} valid={len(all_valid_imgs)} test={len(all_test_imgs)}')\n",
    "\n",
    "# If valid/test are too small, redistribute\n",
    "total = len(all_train_imgs) + len(all_valid_imgs) + len(all_test_imgs)\n",
    "target_valid = int(total * 0.15)\n",
    "target_test = int(total * 0.05)\n",
    "\n",
    "if len(all_valid_imgs) < target_valid * 0.5:\n",
    "    print('Redistributing to 80/15/5 split...')\n",
    "    # Move all back to train first\n",
    "    for imgs_path in [all_valid_imgs, all_test_imgs]:\n",
    "        for img in imgs_path:\n",
    "            stem = Path(img).stem\n",
    "            ext = Path(img).suffix\n",
    "            lbl = img.replace('/images/', '/labels/').replace(ext, '.txt')\n",
    "            shutil.move(img, f'{MERGED_DIR}/train/images/{Path(img).name}')\n",
    "            if os.path.exists(lbl):\n",
    "                shutil.move(lbl, f'{MERGED_DIR}/train/labels/{Path(lbl).name}')\n",
    "    \n",
    "    # Shuffle and split\n",
    "    all_imgs = glob.glob(f'{MERGED_DIR}/train/images/*')\n",
    "    random.seed(42)\n",
    "    random.shuffle(all_imgs)\n",
    "    \n",
    "    valid_imgs = all_imgs[:target_valid]\n",
    "    test_imgs = all_imgs[target_valid:target_valid + target_test]\n",
    "    \n",
    "    for imgs, split in [(valid_imgs, 'valid'), (test_imgs, 'test')]:\n",
    "        for img in imgs:\n",
    "            stem = Path(img).stem\n",
    "            ext = Path(img).suffix\n",
    "            lbl = img.replace('/images/', '/labels/').replace(ext, '.txt')\n",
    "            shutil.move(img, f'{MERGED_DIR}/{split}/images/{Path(img).name}')\n",
    "            if os.path.exists(lbl):\n",
    "                shutil.move(lbl, f'{MERGED_DIR}/{split}/labels/{Path(lbl).name}')\n",
    "\n",
    "# Final counts\n",
    "final_train = len(glob.glob(f'{MERGED_DIR}/train/images/*'))\n",
    "final_valid = len(glob.glob(f'{MERGED_DIR}/valid/images/*'))\n",
    "final_test = len(glob.glob(f'{MERGED_DIR}/test/images/*'))\n",
    "print(f'\\nFinal split: train={final_train} valid={final_valid} test={final_test}')\n",
    "print(f'Total: {final_train + final_valid + final_test} images')\n",
    "\n",
    "# â”€â”€ Create data.yaml â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "data_yaml = {\n",
    "    'names': ['Fighting', 'Fire', 'Gun', 'Knife'],\n",
    "    'nc': 4,\n",
    "    'train': f'{MERGED_DIR}/train/images',\n",
    "    'val': f'{MERGED_DIR}/valid/images',\n",
    "    'test': f'{MERGED_DIR}/test/images',\n",
    "}\n",
    "\n",
    "data_yaml_path = f'{MERGED_DIR}/data.yaml'\n",
    "with open(data_yaml_path, 'w') as f:\n",
    "    yaml.dump(data_yaml, f, default_flow_style=False)\n",
    "\n",
    "print(f'\\nâœ… data.yaml created: {data_yaml_path}')\n",
    "print(yaml.dump(data_yaml, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de28c231",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ¯ Section 3: Fine-Tune Model from `best.pt`\n",
    "\n",
    "### Key differences from training from scratch:\n",
    "- `model = YOLO('best.pt')` instead of `YOLO('yolov8s.pt')`\n",
    "- **Lower learning rate** (0.0005 vs 0.001) â€” preserves learned features\n",
    "- **Fewer epochs** (100 vs 200) â€” model already has a good baseline\n",
    "- **Less aggressive augmentation** â€” don't distort what it already knows\n",
    "- **Freeze early layers** option â€” lock backbone, only train detection head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474bb78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3A: Configure fine-tuning parameters\n",
    "# ============================================================\n",
    "\n",
    "# â”€â”€ Choose your strategy â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# STRATEGY 1: Full fine-tune (recommended - best results)\n",
    "#   - All layers trainable, low learning rate\n",
    "#   - Best when you have significant new data (>500 images)\n",
    "#\n",
    "# STRATEGY 2: Freeze backbone (faster, less risk of forgetting)\n",
    "#   - Only train detection head layers\n",
    "#   - Best when new data is small or very different\n",
    "\n",
    "STRATEGY = 'full'  # Change to 'freeze_backbone' if needed\n",
    "\n",
    "# Training config\n",
    "FINETUNE_CONFIG = {\n",
    "    # â”€â”€ Core â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    'model': MODEL_PATH,           # YOUR trained model, NOT yolov8s.pt\n",
    "    'data': data_yaml_path,        # Merged dataset\n",
    "    'epochs': 100,                 # Fewer epochs needed for fine-tune\n",
    "    'patience': 30,                # Early stopping\n",
    "    'batch': 16,                   # Adjust if OOM\n",
    "    'imgsz': 640,\n",
    "    'device': 0,\n",
    "    \n",
    "    # â”€â”€ Learning Rate (LOWER for fine-tuning) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    'optimizer': 'AdamW',\n",
    "    'lr0': 0.0005,                 # Half of v1's 0.001\n",
    "    'lrf': 0.01,\n",
    "    'weight_decay': 0.0005,\n",
    "    'warmup_epochs': 3,            # Shorter warmup\n",
    "    \n",
    "    # â”€â”€ Augmentation (LIGHTER for fine-tuning) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    'hsv_h': 0.015,\n",
    "    'hsv_s': 0.5,                  # Reduced from 0.7\n",
    "    'hsv_v': 0.3,                  # Reduced from 0.4\n",
    "    'degrees': 5.0,                # Reduced from 10.0\n",
    "    'translate': 0.15,\n",
    "    'scale': 0.4,                  # Reduced from 0.5\n",
    "    'shear': 2.0,                  # Reduced from 5.0\n",
    "    'flipud': 0.0,\n",
    "    'fliplr': 0.5,\n",
    "    'mosaic': 1.0,\n",
    "    'mixup': 0.1,                  # Reduced from 0.15\n",
    "    'copy_paste': 0.1,\n",
    "    'erasing': 0.3,                # Reduced from 0.4\n",
    "    'close_mosaic': 15,\n",
    "    \n",
    "    # â”€â”€ Backbone freeze (Strategy 2 only) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    'freeze': 10 if STRATEGY == 'freeze_backbone' else None,\n",
    "    \n",
    "    # â”€â”€ Other â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    'dropout': 0.05,               # Lower dropout for fine-tune\n",
    "    'save': True,\n",
    "    'save_period': 10,\n",
    "    'plots': True,\n",
    "    'project': '/content/runs',\n",
    "    'name': 'smart_lens_v2',\n",
    "    'exist_ok': True,\n",
    "}\n",
    "\n",
    "print(f'Strategy: {STRATEGY}')\n",
    "print(f'Starting from: {FINETUNE_CONFIG[\"model\"]}')\n",
    "print(f'Dataset: {FINETUNE_CONFIG[\"data\"]}')\n",
    "print(f'Epochs: {FINETUNE_CONFIG[\"epochs\"]}')\n",
    "print(f'Learning rate: {FINETUNE_CONFIG[\"lr0\"]}')\n",
    "if FINETUNE_CONFIG['freeze']:\n",
    "    print(f'Frozen layers: first {FINETUNE_CONFIG[\"freeze\"]} layers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906a6e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3B: START FINE-TUNING\n",
    "# ============================================================\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load YOUR trained model (not a pretrained base model)\n",
    "model = YOLO(FINETUNE_CONFIG['model'])\n",
    "print(f'\\nModel loaded: {FINETUNE_CONFIG[\"model\"]}')\n",
    "print(f'Model type: {model.type}')\n",
    "print(f'Classes: {model.names}')\n",
    "print(f'\\n--- Starting fine-tuning... ---\\n')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Build training args (remove None values)\n",
    "train_args = {k: v for k, v in FINETUNE_CONFIG.items() if v is not None and k != 'model'}\n",
    "\n",
    "results = model.train(**train_args)\n",
    "\n",
    "training_time = (time.time() - start_time) / 60\n",
    "print(f'\\nâœ… Fine-tuning complete! Time: {training_time:.1f} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d4dc45",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“Š Section 4: Evaluate & Compare v1 vs v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcef4e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4A: Validate the fine-tuned model\n",
    "# ============================================================\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load best fine-tuned model\n",
    "best_v2_path = '/content/runs/smart_lens_v2/weights/best.pt'\n",
    "model_v2 = YOLO(best_v2_path)\n",
    "\n",
    "# Validate on merged validation set\n",
    "metrics_v2 = model_v2.val(data=data_yaml_path, split='val')\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'  Fine-tuned Model (v2) Metrics')\n",
    "print(f'{\"=\"*60}')\n",
    "print(f'  mAP50:     {metrics_v2.box.map50:.4f}')\n",
    "print(f'  mAP50-95:  {metrics_v2.box.map:.4f}')\n",
    "print(f'  Precision: {metrics_v2.box.mp:.4f}')\n",
    "print(f'  Recall:    {metrics_v2.box.mr:.4f}')\n",
    "print(f'\\n  Per-class AP50:')\n",
    "for i, cls_name in enumerate(TARGET_CLASSES.values()):\n",
    "    if i < len(metrics_v2.box.ap50):\n",
    "        print(f'    {cls_name:10s}: {metrics_v2.box.ap50[i]:.4f}')\n",
    "print(f'{\"=\"*60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b34165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4B: Compare v1 vs v2 side by side\n",
    "# ============================================================\n",
    "\n",
    "# v1 metrics (from your first training)\n",
    "v1_metrics = {\n",
    "    'mAP50': 0.7255,\n",
    "    'mAP50_95': 0.3322,\n",
    "    'precision': 0.8410,\n",
    "    'recall': 0.6213,\n",
    "}\n",
    "\n",
    "v2_metrics = {\n",
    "    'mAP50': metrics_v2.box.map50,\n",
    "    'mAP50_95': metrics_v2.box.map,\n",
    "    'precision': metrics_v2.box.mp,\n",
    "    'recall': metrics_v2.box.mr,\n",
    "}\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'{\"Metric\":<15} {\"v1 (Original)\":>15} {\"v2 (Fine-tuned)\":>15} {\"Change\":>10}')\n",
    "print(f'{\"â”€\"*60}')\n",
    "for key in ['mAP50', 'mAP50_95', 'precision', 'recall']:\n",
    "    v1 = v1_metrics[key]\n",
    "    v2 = v2_metrics[key]\n",
    "    delta = v2 - v1\n",
    "    arrow = 'â†‘' if delta > 0 else 'â†“' if delta < 0 else 'â†’'\n",
    "    color_delta = f'{arrow} {abs(delta):.4f}'\n",
    "    print(f'{key:<15} {v1:>15.4f} {v2:>15.4f} {color_delta:>10}')\n",
    "print(f'{\"=\"*60}')\n",
    "\n",
    "if v2_metrics['mAP50'] > v1_metrics['mAP50']:\n",
    "    print('\\nğŸ‰ v2 is BETTER than v1! Improvement achieved.')\n",
    "else:\n",
    "    print('\\nâš ï¸ v2 did not improve over v1. Consider:')\n",
    "    print('  - Adding more diverse data')\n",
    "    print('  - Training for more epochs')\n",
    "    print('  - Using freeze_backbone strategy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6337d4",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“¦ Section 5: Export & Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853cbfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5A: Export to ONNX + Save to Drive\n",
    "# ============================================================\n",
    "import json\n",
    "\n",
    "# Export ONNX\n",
    "model_v2.export(format='onnx', simplify=True)\n",
    "print('âœ… ONNX export complete')\n",
    "\n",
    "# Create output directory\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "output_dir = f'{DRIVE_DIR}/models/smart_lens_v2_{timestamp}'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Copy model files\n",
    "shutil.copy(best_v2_path, f'{output_dir}/best.pt')\n",
    "shutil.copy(best_v2_path.replace('.pt', '.onnx'), f'{output_dir}/best.onnx')\n",
    "shutil.copy('/content/runs/smart_lens_v2/weights/last.pt', f'{output_dir}/last.pt')\n",
    "\n",
    "# Save metrics\n",
    "metrics_data = {\n",
    "    'mAP50': float(metrics_v2.box.map50),\n",
    "    'mAP50_95': float(metrics_v2.box.map),\n",
    "    'precision': float(metrics_v2.box.mp),\n",
    "    'recall': float(metrics_v2.box.mr),\n",
    "    'training_time_min': round(training_time, 1),\n",
    "    'timestamp': timestamp,\n",
    "    'base_model': 'smart_lens_v1 (fine-tuned)',\n",
    "    'strategy': STRATEGY,\n",
    "    'total_images': stats['total_images'],\n",
    "    'classes': list(TARGET_CLASSES.values()),\n",
    "    'per_class_annotations': dict(stats['per_class']),\n",
    "    'v1_comparison': {\n",
    "        'mAP50_delta': float(v2_metrics['mAP50'] - v1_metrics['mAP50']),\n",
    "        'recall_delta': float(v2_metrics['recall'] - v1_metrics['recall']),\n",
    "    }\n",
    "}\n",
    "with open(f'{output_dir}/metrics.json', 'w') as f:\n",
    "    json.dump(metrics_data, f, indent=2)\n",
    "\n",
    "# Save training config\n",
    "with open(f'{output_dir}/training_config.json', 'w') as f:\n",
    "    json.dump(FINETUNE_CONFIG, f, indent=2, default=str)\n",
    "\n",
    "print(f'\\nâœ… All files saved to Google Drive:')\n",
    "print(f'   {output_dir}/')\n",
    "for f in os.listdir(output_dir):\n",
    "    size = os.path.getsize(f'{output_dir}/{f}') / 1024 / 1024\n",
    "    print(f'   â”œâ”€â”€ {f} ({size:.1f} MB)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2579cc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5B: Download model files to local machine\n",
    "# ============================================================\n",
    "from google.colab import files\n",
    "\n",
    "# Download best.pt\n",
    "files.download(f'{output_dir}/best.pt')\n",
    "print('Download best.pt â€” place it in:')\n",
    "print('  Smart-Lens-FYP/trained_models/smart_lens_v2_XXXX/best.pt')\n",
    "\n",
    "# Optionally download ONNX too\n",
    "# files.download(f'{output_dir}/best.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73e9836",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ’¡ Section 6: Quick Test on Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db74d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6: Quick visual test on test images\n",
    "# ============================================================\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "test_images = glob.glob(f'{MERGED_DIR}/test/images/*')[:8]\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, img_path in enumerate(test_images):\n",
    "    results = model_v2.predict(img_path, conf=0.4, verbose=False)\n",
    "    annotated = results[0].plot()\n",
    "    axes[i].imshow(annotated[:, :, ::-1])  # BGR to RGB\n",
    "    axes[i].set_title(Path(img_path).stem[:30], fontsize=8)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Smart Lens v2 â€” Test Predictions', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/test_predictions.png', dpi=150)\n",
    "plt.show()\n",
    "print('Test predictions saved to Drive.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fe71fb",
   "metadata": {},
   "source": [
    "---\n",
    "## âœ… Done! Next Steps\n",
    "\n",
    "1. **Download `best.pt`** from the output above (or from Google Drive)\n",
    "2. **Place it** in `Smart-Lens-FYP/trained_models/smart_lens_v2_XXXX/`\n",
    "3. **Test locally** with:\n",
    "   ```bash\n",
    "   python smart_lens_v2.py --source 0 --model trained_models/smart_lens_v2_XXXX/best.pt\n",
    "   ```\n",
    "\n",
    "### If results still need improvement:\n",
    "- **Add more data**: Search Roboflow Universe for more gun/fighting datasets\n",
    "- **Run this notebook again**: Upload the new `best.pt` from v2 and fine-tune to v3\n",
    "- **Try YOLOv8m**: Change base model for +3-5% mAP (slower but more accurate)\n",
    "- **Increase epochs**: Set to 150-200 if the model is still improving at epoch 100"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
