{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84ebaab1",
   "metadata": {},
   "source": [
    "# üîç Smart Lens ‚Äî YOLOv8 Threat Detection Model Training\n",
    "\n",
    "**Complete training pipeline for the Smart Lens CCTV Surveillance System**\n",
    "\n",
    "---\n",
    "\n",
    "### What this notebook does:\n",
    "1. ‚úÖ Connects Google Drive (saves everything permanently)\n",
    "2. ‚úÖ Clones your GitHub repo\n",
    "3. ‚úÖ Downloads Roboflow labeled dataset (750 images)\n",
    "4. ‚úÖ Explores & validates the dataset\n",
    "5. ‚úÖ Trains YOLOv8 with optimized settings for small datasets\n",
    "6. ‚úÖ Evaluates model performance (mAP, confusion matrix, PR curves)\n",
    "7. ‚úÖ Tests on sample images/video\n",
    "8. ‚úÖ Exports best model for deployment\n",
    "9. ‚úÖ Saves everything to Drive + pushes to GitHub\n",
    "\n",
    "### ‚ö° Before running:\n",
    "- Go to **Runtime ‚Üí Change runtime type ‚Üí T4 GPU ‚Üí Save**\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Training Strategy for 750 Images\n",
    "\n",
    "| Setting | Value | Why |\n",
    "|---------|-------|-----|\n",
    "| **Model** | YOLOv8s (small) | Best accuracy/speed balance for small datasets |\n",
    "| **Epochs** | 200 | Small dataset needs more epochs to converge |\n",
    "| **Early Stopping** | patience=50 | Auto-stops if no improvement for 50 epochs |\n",
    "| **Image Size** | 640 | Standard for YOLO, good for CCTV frames |\n",
    "| **Batch Size** | 16 | Optimal for T4 GPU (16GB VRAM) |\n",
    "| **Augmentation** | Heavy | Critical for small datasets ‚Äî prevents overfitting |\n",
    "| **Optimizer** | AdamW | Better convergence than SGD for small data |\n",
    "| **Learning Rate** | 0.001 ‚Üí cosine decay | Smooth convergence |\n",
    "| **Pretrained** | COCO weights | Transfer learning is essential with 750 images |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de26f6c3",
   "metadata": {},
   "source": [
    "---\n",
    "## üì¶ Step 0: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0825d85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 0.1 ‚Äî Mount Google Drive (permanent storage)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create project folder on Drive\n",
    "import os\n",
    "DRIVE_PROJECT = '/content/drive/MyDrive/Smart-Lens-FYP'\n",
    "DRIVE_MODELS  = f'{DRIVE_PROJECT}/trained_models'\n",
    "DRIVE_RESULTS = f'{DRIVE_PROJECT}/training_results'\n",
    "DRIVE_DATASET = f'{DRIVE_PROJECT}/dataset'\n",
    "\n",
    "for d in [DRIVE_PROJECT, DRIVE_MODELS, DRIVE_RESULTS, DRIVE_DATASET]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print('‚úÖ Google Drive mounted!')\n",
    "print(f'üìÇ Project folder: {DRIVE_PROJECT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238f47ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 0.2 ‚Äî Verify GPU\n",
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError('‚ùå No GPU! Go to Runtime ‚Üí Change runtime type ‚Üí T4 GPU')\n",
    "\n",
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "print(f'‚úÖ GPU: {gpu_name} ({gpu_mem:.1f} GB VRAM)')\n",
    "print(f'   PyTorch: {torch.__version__}')\n",
    "print(f'   CUDA: {torch.version.cuda}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c22cbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 0.3 ‚Äî Install dependencies\n",
    "!pip install -q ultralytics roboflow\n",
    "\n",
    "import ultralytics\n",
    "ultralytics.checks()\n",
    "print('‚úÖ All dependencies installed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a065f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 0.4 ‚Äî Clone GitHub repo\n",
    "import os\n",
    "\n",
    "REPO_URL = 'https://github.com/Alee-Razaa/Smart-Lens-FYP.git'\n",
    "REPO_DIR = '/content/Smart-Lens-FYP'\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone {REPO_URL} {REPO_DIR}\n",
    "else:\n",
    "    !cd {REPO_DIR} && git pull\n",
    "\n",
    "# Configure git\n",
    "!git config --global user.name 'Ali Raza Memon'\n",
    "!git config --global user.email 'alirazamemon.bsaif22@iba-suk.edu.pk'\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "print(f'‚úÖ Repo ready at: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17746f76",
   "metadata": {},
   "source": [
    "---\n",
    "## üì• Step 1: Download & Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287829ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1.1 ‚Äî Download dataset from Roboflow\n",
    "from roboflow import Roboflow\n",
    "import shutil\n",
    "\n",
    "# Check if dataset already exists on Drive (skip re-download)\n",
    "DATASET_DIR = '/content/dataset'\n",
    "\n",
    "if os.path.exists(f'{DRIVE_DATASET}/data.yaml'):\n",
    "    print('üì¶ Dataset found on Drive! Copying to runtime (faster)...')\n",
    "    if os.path.exists(DATASET_DIR):\n",
    "        shutil.rmtree(DATASET_DIR)\n",
    "    shutil.copytree(DRIVE_DATASET, DATASET_DIR)\n",
    "    print(f'‚úÖ Dataset loaded from Drive')\n",
    "else:\n",
    "    print('üì• Downloading dataset from Roboflow...')\n",
    "    rf = Roboflow(api_key='7QsEv54uizzlrvPZ972Z')\n",
    "    project = rf.workspace('fpy').project('smart-survellaince-lens-2')\n",
    "    version = project.version(1)\n",
    "    dataset = version.download('yolov8', location=DATASET_DIR)\n",
    "\n",
    "    # Backup to Drive\n",
    "    print('üíæ Backing up dataset to Google Drive...')\n",
    "    if os.path.exists(DRIVE_DATASET):\n",
    "        shutil.rmtree(DRIVE_DATASET)\n",
    "    shutil.copytree(DATASET_DIR, DRIVE_DATASET)\n",
    "    print(f'‚úÖ Dataset downloaded & backed up to Drive')\n",
    "\n",
    "# Show structure\n",
    "!find {DATASET_DIR} -type d | head -20\n",
    "print(f'\\nüìÑ data.yaml contents:')\n",
    "!cat {DATASET_DIR}/data.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a993f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1.2 ‚Äî Dataset statistics & analysis\n",
    "import yaml\n",
    "import glob\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load data.yaml\n",
    "with open(f'{DATASET_DIR}/data.yaml', 'r') as f:\n",
    "    data_config = yaml.safe_load(f)\n",
    "\n",
    "class_names = data_config['names']\n",
    "num_classes = len(class_names) if isinstance(class_names, list) else len(class_names.values())\n",
    "\n",
    "# Handle both list and dict formats for class names\n",
    "if isinstance(class_names, dict):\n",
    "    class_list = [class_names[i] for i in sorted(class_names.keys())]\n",
    "else:\n",
    "    class_list = class_names\n",
    "\n",
    "print(f'üìä DATASET SUMMARY')\n",
    "print(f'===================')\n",
    "print(f'Classes ({num_classes}): {class_list}')\n",
    "\n",
    "# Count images per split\n",
    "splits = {}\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    img_path = f'{DATASET_DIR}/{split}/images'\n",
    "    if os.path.exists(img_path):\n",
    "        count = len(glob.glob(f'{img_path}/*'))\n",
    "        splits[split] = count\n",
    "        print(f'  {split}: {count} images')\n",
    "\n",
    "total = sum(splits.values())\n",
    "print(f'  TOTAL: {total} images')\n",
    "\n",
    "# Count labels per class\n",
    "all_labels = []\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    label_path = f'{DATASET_DIR}/{split}/labels'\n",
    "    if os.path.exists(label_path):\n",
    "        for txt_file in glob.glob(f'{label_path}/*.txt'):\n",
    "            with open(txt_file, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if parts:\n",
    "                        all_labels.append(int(parts[0]))\n",
    "\n",
    "label_counts = Counter(all_labels)\n",
    "print(f'\\nüìä LABEL DISTRIBUTION')\n",
    "print(f'=====================')\n",
    "for cls_id in sorted(label_counts.keys()):\n",
    "    name = class_list[cls_id] if cls_id < len(class_list) else f'class_{cls_id}'\n",
    "    count = label_counts[cls_id]\n",
    "    bar = '‚ñà' * (count // 5)\n",
    "    print(f'  [{cls_id}] {name:20s}: {count:5d} {bar}')\n",
    "\n",
    "print(f'  TOTAL annotations: {len(all_labels)}')\n",
    "\n",
    "# Plot distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Split distribution\n",
    "axes[0].bar(splits.keys(), splits.values(), color=['#2ecc71', '#3498db', '#e74c3c'])\n",
    "axes[0].set_title('Images per Split', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Count')\n",
    "for i, (k, v) in enumerate(splits.items()):\n",
    "    axes[0].text(i, v + 5, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Class distribution\n",
    "cls_names_sorted = [class_list[i] if i < len(class_list) else f'cls_{i}' for i in sorted(label_counts.keys())]\n",
    "cls_counts_sorted = [label_counts[i] for i in sorted(label_counts.keys())]\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, len(cls_names_sorted)))\n",
    "axes[1].barh(cls_names_sorted, cls_counts_sorted, color=colors)\n",
    "axes[1].set_title('Annotations per Class', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Count')\n",
    "for i, v in enumerate(cls_counts_sorted):\n",
    "    axes[1].text(v + 2, i, str(v), va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{DRIVE_RESULTS}/dataset_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('\\nüíæ Saved: dataset_analysis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dedcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1.3 ‚Äî Visualize sample images with labels\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Get random training images\n",
    "train_images = glob.glob(f'{DATASET_DIR}/train/images/*')\n",
    "samples = random.sample(train_images, min(9, len(train_images)))\n",
    "\n",
    "# Color map for classes\n",
    "COLORS = [(0,255,0), (0,0,255), (255,0,0), (255,255,0), (255,0,255),\n",
    "          (0,255,255), (128,0,255), (255,128,0)]\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 18))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, img_path in enumerate(samples):\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    # Read corresponding label\n",
    "    label_path = img_path.replace('/images/', '/labels/')\n",
    "    label_path = os.path.splitext(label_path)[0] + '.txt'\n",
    "\n",
    "    if os.path.exists(label_path):\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 5:\n",
    "                    cls_id = int(parts[0])\n",
    "                    xc, yc, bw, bh = [float(x) for x in parts[1:5]]\n",
    "\n",
    "                    # Convert YOLO format to pixel coordinates\n",
    "                    x1 = int((xc - bw/2) * w)\n",
    "                    y1 = int((yc - bh/2) * h)\n",
    "                    x2 = int((xc + bw/2) * w)\n",
    "                    y2 = int((yc + bh/2) * h)\n",
    "\n",
    "                    color = COLORS[cls_id % len(COLORS)]\n",
    "                    cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "                    label = class_list[cls_id] if cls_id < len(class_list) else f'cls_{cls_id}'\n",
    "                    cv2.putText(img, label, (x1, y1-8), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "\n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].set_title(os.path.basename(img_path), fontsize=9)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('üì∏ Sample Training Images with Annotations', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{DRIVE_RESULTS}/sample_images.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0492f5",
   "metadata": {},
   "source": [
    "---\n",
    "## üîß Step 2: Fix data.yaml paths for Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc6d65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2.1 ‚Äî Update data.yaml with correct Colab paths\n",
    "import yaml\n",
    "\n",
    "with open(f'{DATASET_DIR}/data.yaml', 'r') as f:\n",
    "    data_config = yaml.safe_load(f)\n",
    "\n",
    "# Fix paths for Colab\n",
    "data_config['train'] = f'{DATASET_DIR}/train/images'\n",
    "data_config['val']   = f'{DATASET_DIR}/valid/images'\n",
    "\n",
    "# Add test path if exists\n",
    "if os.path.exists(f'{DATASET_DIR}/test/images'):\n",
    "    data_config['test'] = f'{DATASET_DIR}/test/images'\n",
    "\n",
    "# Save updated config\n",
    "DATA_YAML = f'{DATASET_DIR}/data.yaml'\n",
    "with open(DATA_YAML, 'w') as f:\n",
    "    yaml.dump(data_config, f, default_flow_style=False)\n",
    "\n",
    "print('‚úÖ data.yaml updated:')\n",
    "!cat {DATA_YAML}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e66bce4",
   "metadata": {},
   "source": [
    "---\n",
    "## üöÄ Step 3: Train YOLOv8 Model\n",
    "\n",
    "### Training Strategy for 750 Images\n",
    "\n",
    "With only **750 images**, we must:\n",
    "1. **Use pretrained weights** (COCO) ‚Äî transfer learning is critical\n",
    "2. **Heavy augmentation** ‚Äî artificially increase dataset diversity\n",
    "3. **More epochs** (200) ‚Äî small datasets need more passes to learn\n",
    "4. **Early stopping** (patience=50) ‚Äî auto-stop if overfitting\n",
    "5. **YOLOv8s** (small) ‚Äî large models overfit on small data\n",
    "\n",
    "| Epochs Guide for Dataset Size |\n",
    "|------|\n",
    "| < 500 images ‚Üí 250-300 epochs |\n",
    "| 500-1000 images ‚Üí 150-200 epochs |\n",
    "| 1000-5000 images ‚Üí 100-150 epochs |\n",
    "| 5000+ images ‚Üí 50-100 epochs |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69ef9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3.1 ‚Äî Configure training parameters\n",
    "\n",
    "# ============================================================\n",
    "#  TRAINING CONFIGURATION ‚Äî Optimized for 750 images\n",
    "# ============================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    'model': 'yolov8s.pt',          # Small model ‚Äî best for <1000 images\n",
    "                                     # Options: yolov8n.pt (nano/fastest),\n",
    "                                     #          yolov8s.pt (small/balanced) ‚úÖ\n",
    "                                     #          yolov8m.pt (medium/more accurate but risk overfit)\n",
    "\n",
    "    # Training\n",
    "    'epochs': 200,                   # 200 epochs for 750 images\n",
    "    'patience': 50,                  # Early stopping ‚Äî stops if no improvement for 50 epochs\n",
    "    'batch': 16,                     # Batch size ‚Äî optimal for T4 16GB VRAM\n",
    "    'imgsz': 640,                    # Image size ‚Äî standard for YOLO\n",
    "    'device': 0,                     # GPU device\n",
    "\n",
    "    # Optimizer\n",
    "    'optimizer': 'AdamW',            # Better than SGD for small datasets\n",
    "    'lr0': 0.001,                    # Initial learning rate\n",
    "    'lrf': 0.01,                     # Final LR = lr0 * lrf (cosine decay)\n",
    "    'weight_decay': 0.0005,          # L2 regularization\n",
    "    'warmup_epochs': 5,              # Warmup for stable start\n",
    "\n",
    "    # Augmentation (HEAVY for small dataset)\n",
    "    'hsv_h': 0.015,                  # Hue augmentation\n",
    "    'hsv_s': 0.7,                    # Saturation augmentation\n",
    "    'hsv_v': 0.4,                    # Value/brightness augmentation\n",
    "    'degrees': 10.0,                 # Rotation ¬±10¬∞\n",
    "    'translate': 0.2,                # Translation ¬±20%\n",
    "    'scale': 0.5,                    # Scale ¬±50%\n",
    "    'shear': 5.0,                    # Shear ¬±5¬∞\n",
    "    'flipud': 0.0,                   # No vertical flip (surveillance is upright)\n",
    "    'fliplr': 0.5,                   # Horizontal flip 50%\n",
    "    'mosaic': 1.0,                   # Mosaic augmentation (combine 4 images)\n",
    "    'mixup': 0.15,                   # MixUp 15% (blend 2 images)\n",
    "    'copy_paste': 0.1,               # Copy-Paste augmentation 10%\n",
    "    'erasing': 0.4,                  # Random Erasing 40% (simulates occlusion)\n",
    "\n",
    "    # Regularization\n",
    "    'dropout': 0.1,                  # Light dropout to prevent overfitting\n",
    "    'close_mosaic': 20,              # Disable mosaic for last 20 epochs (fine-tune)\n",
    "\n",
    "    # Saving\n",
    "    'project': '/content/runs',\n",
    "    'name': 'smart_lens_v1',\n",
    "    'save': True,\n",
    "    'save_period': 25,               # Save checkpoint every 25 epochs\n",
    "    'plots': True,                   # Generate training plots\n",
    "    'exist_ok': True,\n",
    "}\n",
    "\n",
    "print('‚úÖ Training Configuration:')\n",
    "print(f'   Model:      {CONFIG[\"model\"]}')\n",
    "print(f'   Epochs:     {CONFIG[\"epochs\"]} (early stop patience={CONFIG[\"patience\"]})')\n",
    "print(f'   Batch Size: {CONFIG[\"batch\"]}')\n",
    "print(f'   Image Size: {CONFIG[\"imgsz\"]}')\n",
    "print(f'   Optimizer:  {CONFIG[\"optimizer\"]} (lr={CONFIG[\"lr0\"]})')\n",
    "print(f'   Augmentation: HEAVY (mosaic={CONFIG[\"mosaic\"]}, mixup={CONFIG[\"mixup\"]}, erasing={CONFIG[\"erasing\"]})')\n",
    "print(f'\\n‚è±Ô∏è Estimated training time: ~45-90 minutes on T4 GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33f85a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3.2 ‚Äî üöÄ START TRAINING (run this and wait)\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "\n",
    "print('üöÄ Starting YOLOv8 training...')\n",
    "print(f'   Dataset: {DATA_YAML}')\n",
    "print(f'   This will take ~45-90 minutes on T4 GPU')\n",
    "print('=' * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load pretrained model\n",
    "model = YOLO(CONFIG['model'])\n",
    "\n",
    "# Train\n",
    "results = model.train(\n",
    "    data=DATA_YAML,\n",
    "    epochs=CONFIG['epochs'],\n",
    "    patience=CONFIG['patience'],\n",
    "    batch=CONFIG['batch'],\n",
    "    imgsz=CONFIG['imgsz'],\n",
    "    device=CONFIG['device'],\n",
    "    optimizer=CONFIG['optimizer'],\n",
    "    lr0=CONFIG['lr0'],\n",
    "    lrf=CONFIG['lrf'],\n",
    "    weight_decay=CONFIG['weight_decay'],\n",
    "    warmup_epochs=CONFIG['warmup_epochs'],\n",
    "    hsv_h=CONFIG['hsv_h'],\n",
    "    hsv_s=CONFIG['hsv_s'],\n",
    "    hsv_v=CONFIG['hsv_v'],\n",
    "    degrees=CONFIG['degrees'],\n",
    "    translate=CONFIG['translate'],\n",
    "    scale=CONFIG['scale'],\n",
    "    shear=CONFIG['shear'],\n",
    "    flipud=CONFIG['flipud'],\n",
    "    fliplr=CONFIG['fliplr'],\n",
    "    mosaic=CONFIG['mosaic'],\n",
    "    mixup=CONFIG['mixup'],\n",
    "    copy_paste=CONFIG['copy_paste'],\n",
    "    erasing=CONFIG['erasing'],\n",
    "    dropout=CONFIG['dropout'],\n",
    "    close_mosaic=CONFIG['close_mosaic'],\n",
    "    project=CONFIG['project'],\n",
    "    name=CONFIG['name'],\n",
    "    save=CONFIG['save'],\n",
    "    save_period=CONFIG['save_period'],\n",
    "    plots=CONFIG['plots'],\n",
    "    exist_ok=CONFIG['exist_ok'],\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print('\\n' + '=' * 60)\n",
    "print(f'‚úÖ Training complete! Time: {elapsed/60:.1f} minutes')\n",
    "print(f'üìÇ Results saved to: {CONFIG[\"project\"]}/{CONFIG[\"name\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f73f4c",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Step 4: Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6208c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4.1 ‚Äî Show training curves\n",
    "from IPython.display import Image, display\n",
    "\n",
    "RESULTS_DIR = f'{CONFIG[\"project\"]}/{CONFIG[\"name\"]}'\n",
    "\n",
    "# Training curves\n",
    "print('üìà TRAINING CURVES')\n",
    "print('=' * 40)\n",
    "if os.path.exists(f'{RESULTS_DIR}/results.png'):\n",
    "    display(Image(filename=f'{RESULTS_DIR}/results.png', width=900))\n",
    "else:\n",
    "    print('‚ö†Ô∏è results.png not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5142c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4.2 ‚Äî Confusion Matrix\n",
    "print('üî¢ CONFUSION MATRIX')\n",
    "print('=' * 40)\n",
    "for fname in ['confusion_matrix.png', 'confusion_matrix_normalized.png']:\n",
    "    fpath = f'{RESULTS_DIR}/{fname}'\n",
    "    if os.path.exists(fpath):\n",
    "        print(f'\\n{fname}:')\n",
    "        display(Image(filename=fpath, width=700))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0028e642",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4.3 ‚Äî Precision-Recall & F1 Curves\n",
    "print('üìä PRECISION-RECALL & F1 CURVES')\n",
    "print('=' * 40)\n",
    "for fname in ['PR_curve.png', 'P_curve.png', 'R_curve.png', 'F1_curve.png']:\n",
    "    fpath = f'{RESULTS_DIR}/{fname}'\n",
    "    if os.path.exists(fpath):\n",
    "        print(f'\\n{fname}:')\n",
    "        display(Image(filename=fpath, width=700))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a36747",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4.4 ‚Äî Validate on validation set (detailed metrics)\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load best weights\n",
    "best_model = YOLO(f'{RESULTS_DIR}/weights/best.pt')\n",
    "\n",
    "# Run validation\n",
    "val_results = best_model.val(\n",
    "    data=DATA_YAML,\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    device=0,\n",
    "    plots=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print('\\n' + '=' * 60)\n",
    "print('üìä VALIDATION RESULTS')\n",
    "print('=' * 60)\n",
    "print(f'  mAP50:      {val_results.box.map50:.4f}')\n",
    "print(f'  mAP50-95:   {val_results.box.map:.4f}')\n",
    "print(f'  Precision:  {val_results.box.mp:.4f}')\n",
    "print(f'  Recall:     {val_results.box.mr:.4f}')\n",
    "print('\\n  Per-class AP50:')\n",
    "for i, ap in enumerate(val_results.box.ap50):\n",
    "    name = class_list[i] if i < len(class_list) else f'class_{i}'\n",
    "    bar = '‚ñà' * int(ap * 30)\n",
    "    print(f'    [{i}] {name:20s}: {ap:.4f} {bar}')\n",
    "\n",
    "# Quality assessment\n",
    "map50 = val_results.box.map50\n",
    "print('\\n' + '=' * 60)\n",
    "if map50 >= 0.7:\n",
    "    print(f'‚úÖ GOOD! mAP50={map50:.2%} ‚Äî Model is performing well')\n",
    "elif map50 >= 0.5:\n",
    "    print(f'‚ö†Ô∏è DECENT. mAP50={map50:.2%} ‚Äî Consider more data or fine-tuning')\n",
    "else:\n",
    "    print(f'‚ùå NEEDS IMPROVEMENT. mAP50={map50:.2%} ‚Äî See recommendations below')\n",
    "    print('   ‚Üí Add more labeled images (target 1500+)')\n",
    "    print('   ‚Üí Check label quality in Roboflow')\n",
    "    print('   ‚Üí Try yolov8m.pt or increase epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbacf2d",
   "metadata": {},
   "source": [
    "---\n",
    "## üß™ Step 5: Test Model on Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7abe159",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5.1 ‚Äî Run inference on validation images\n",
    "import glob\n",
    "import random\n",
    "\n",
    "# Get validation images\n",
    "val_images = glob.glob(f'{DATASET_DIR}/valid/images/*')\n",
    "test_images = glob.glob(f'{DATASET_DIR}/test/images/*') if os.path.exists(f'{DATASET_DIR}/test/images') else []\n",
    "all_test = val_images + test_images\n",
    "\n",
    "# Pick random samples\n",
    "samples = random.sample(all_test, min(12, len(all_test)))\n",
    "\n",
    "# Run inference\n",
    "results = best_model.predict(\n",
    "    source=samples,\n",
    "    conf=0.4,\n",
    "    iou=0.5,\n",
    "    save=True,\n",
    "    project='/content/runs/predict',\n",
    "    name='test_samples',\n",
    "    exist_ok=True\n",
    ")\n",
    "\n",
    "# Display results\n",
    "fig, axes = plt.subplots(3, 4, figsize=(24, 16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "pred_dir = '/content/runs/predict/test_samples'\n",
    "pred_images = sorted(glob.glob(f'{pred_dir}/*'))[:12]\n",
    "\n",
    "for idx, img_path in enumerate(pred_images):\n",
    "    if idx >= 12:\n",
    "        break\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "# Hide unused axes\n",
    "for idx in range(len(pred_images), 12):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('üîç Model Predictions on Test Images', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{DRIVE_RESULTS}/test_predictions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'\\n‚úÖ Inference complete! {len(pred_images)} images processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7564de12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5.2 ‚Äî Speed benchmark (FPS test)\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Warm up GPU\n",
    "dummy = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)\n",
    "for _ in range(5):\n",
    "    best_model.predict(dummy, verbose=False)\n",
    "\n",
    "# Benchmark\n",
    "times = []\n",
    "for _ in range(50):\n",
    "    start = time.time()\n",
    "    best_model.predict(dummy, verbose=False)\n",
    "    times.append(time.time() - start)\n",
    "\n",
    "avg_ms = np.mean(times) * 1000\n",
    "fps = 1000 / avg_ms\n",
    "\n",
    "print(f'‚ö° SPEED BENCHMARK (T4 GPU)')\n",
    "print(f'==========================')\n",
    "print(f'  Average inference: {avg_ms:.1f} ms per frame')\n",
    "print(f'  FPS: {fps:.1f} frames/second')\n",
    "print(f'  Min: {np.min(times)*1000:.1f} ms | Max: {np.max(times)*1000:.1f} ms')\n",
    "\n",
    "if fps >= 30:\n",
    "    print(f'\\n‚úÖ Real-time capable! ({fps:.0f} FPS > 30 FPS requirement)')\n",
    "elif fps >= 15:\n",
    "    print(f'\\n‚ö†Ô∏è Near real-time ({fps:.0f} FPS) ‚Äî acceptable for surveillance')\n",
    "else:\n",
    "    print(f'\\n‚ùå Below real-time ({fps:.0f} FPS) ‚Äî consider yolov8n.pt for speed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6517ab69",
   "metadata": {},
   "source": [
    "---\n",
    "## üì¶ Step 6: Export Model for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8360e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6.1 ‚Äî Export best model (PyTorch + ONNX)\n",
    "import shutil\n",
    "\n",
    "BEST_PT = f'{RESULTS_DIR}/weights/best.pt'\n",
    "LAST_PT = f'{RESULTS_DIR}/weights/last.pt'\n",
    "\n",
    "# Export to ONNX (for production deployment)\n",
    "print('üì¶ Exporting model to ONNX...')\n",
    "best_model.export(format='onnx', imgsz=640, simplify=True)\n",
    "BEST_ONNX = BEST_PT.replace('.pt', '.onnx')\n",
    "\n",
    "print(f'\\n‚úÖ Models exported:')\n",
    "print(f'  PyTorch:  {BEST_PT} ({os.path.getsize(BEST_PT)/1e6:.1f} MB)')\n",
    "if os.path.exists(BEST_ONNX):\n",
    "    print(f'  ONNX:     {BEST_ONNX} ({os.path.getsize(BEST_ONNX)/1e6:.1f} MB)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc614ff7",
   "metadata": {},
   "source": [
    "---\n",
    "## üíæ Step 7: Save Everything to Drive + Push to GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe36fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7.1 ‚Äî Save trained model + results to Google Drive\n",
    "import shutil\n",
    "import datetime\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M')\n",
    "model_name = f'smart_lens_v1_{timestamp}'\n",
    "\n",
    "# Save best model to Drive\n",
    "drive_model_path = f'{DRIVE_MODELS}/{model_name}'\n",
    "os.makedirs(drive_model_path, exist_ok=True)\n",
    "\n",
    "# Copy weights\n",
    "shutil.copy2(BEST_PT, f'{drive_model_path}/best.pt')\n",
    "shutil.copy2(LAST_PT, f'{drive_model_path}/last.pt')\n",
    "if os.path.exists(BEST_ONNX):\n",
    "    shutil.copy2(BEST_ONNX, f'{drive_model_path}/best.onnx')\n",
    "\n",
    "# Copy all training results/plots\n",
    "for fname in os.listdir(RESULTS_DIR):\n",
    "    fpath = f'{RESULTS_DIR}/{fname}'\n",
    "    if os.path.isfile(fpath):\n",
    "        shutil.copy2(fpath, f'{DRIVE_RESULTS}/{fname}')\n",
    "\n",
    "# Save training config\n",
    "import json\n",
    "with open(f'{drive_model_path}/training_config.json', 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "# Save validation metrics\n",
    "metrics = {\n",
    "    'mAP50': float(val_results.box.map50),\n",
    "    'mAP50_95': float(val_results.box.map),\n",
    "    'precision': float(val_results.box.mp),\n",
    "    'recall': float(val_results.box.mr),\n",
    "    'training_time_min': round(elapsed / 60, 1),\n",
    "    'fps': round(fps, 1),\n",
    "    'timestamp': timestamp,\n",
    "    'classes': class_list if isinstance(class_list, list) else list(class_list),\n",
    "}\n",
    "with open(f'{drive_model_path}/metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f'\\n‚úÖ SAVED TO GOOGLE DRIVE')\n",
    "print(f'========================')\n",
    "print(f'  üìÇ Model:   {drive_model_path}/')\n",
    "print(f'  üìÇ Results: {DRIVE_RESULTS}/')\n",
    "print(f'\\n  Files saved:')\n",
    "for f in os.listdir(drive_model_path):\n",
    "    size = os.path.getsize(f'{drive_model_path}/{f}') / 1e6\n",
    "    print(f'    üìÑ {f} ({size:.1f} MB)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186fb397",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7.2 ‚Äî Push training results to GitHub\n",
    "import json\n",
    "\n",
    "# Save a lightweight results summary to the repo (no large model files)\n",
    "os.chdir(REPO_DIR)\n",
    "\n",
    "# Create results directory in repo\n",
    "os.makedirs(f'{REPO_DIR}/results', exist_ok=True)\n",
    "\n",
    "# Save metrics\n",
    "with open(f'{REPO_DIR}/results/metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "# Copy key plots (small files only)\n",
    "for fname in ['results.png', 'confusion_matrix.png', 'PR_curve.png', 'F1_curve.png']:\n",
    "    src = f'{RESULTS_DIR}/{fname}'\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy2(src, f'{REPO_DIR}/results/{fname}')\n",
    "\n",
    "# Commit and push\n",
    "!cd {REPO_DIR} && git add -A\n",
    "!cd {REPO_DIR} && git status\n",
    "!cd {REPO_DIR} && git commit -m \"Add YOLOv8 training results ‚Äî mAP50={metrics['mAP50']:.4f}\"\n",
    "!cd {REPO_DIR} && git push origin master\n",
    "\n",
    "print(f'\\n‚úÖ Results pushed to GitHub!')\n",
    "print(f'   mAP50: {metrics[\"mAP50\"]:.4f}')\n",
    "print(f'   Repo: https://github.com/Alee-Razaa/Smart-Lens-FYP')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c51240",
   "metadata": {},
   "source": [
    "---\n",
    "## üìã Step 8: Training Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eae512",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 8.1 ‚Äî Print final summary\n",
    "\n",
    "print('=' * 70)\n",
    "print('üîç SMART LENS ‚Äî TRAINING SUMMARY')\n",
    "print('=' * 70)\n",
    "print(f'''\n",
    "  üìä Dataset:        {total} images, {num_classes} classes\n",
    "  ü§ñ Model:          YOLOv8s (pretrained COCO ‚Üí fine-tuned)\n",
    "  ‚è±Ô∏è  Training Time:  {elapsed/60:.1f} minutes\n",
    "  \n",
    "  üìà METRICS:\n",
    "     mAP50:          {metrics[\"mAP50\"]:.4f} ({metrics[\"mAP50\"]:.1%})\n",
    "     mAP50-95:       {metrics[\"mAP50_95\"]:.4f}\n",
    "     Precision:      {metrics[\"precision\"]:.4f}\n",
    "     Recall:         {metrics[\"recall\"]:.4f}\n",
    "     FPS:            {metrics[\"fps\"]} frames/sec\n",
    "  \n",
    "  üíæ SAVED TO:\n",
    "     Google Drive:   {drive_model_path}/\n",
    "     GitHub:         https://github.com/Alee-Razaa/Smart-Lens-FYP\n",
    "  \n",
    "  üîë TO LOAD THIS MODEL LATER:\n",
    "     from ultralytics import YOLO\n",
    "     model = YOLO('{drive_model_path}/best.pt')\n",
    "''')\n",
    "\n",
    "# Recommendations based on results\n",
    "print('  üìå NEXT STEPS:')\n",
    "if metrics['mAP50'] < 0.5:\n",
    "    print('     ‚ùå mAP is low. Recommendations:')\n",
    "    print('        1. Add more labeled images (target 1500+) on Roboflow')\n",
    "    print('        2. Review label quality ‚Äî remove bad annotations')\n",
    "    print('        3. Balance classes ‚Äî under-represented classes need more samples')\n",
    "    print('        4. Try yolov8m.pt with lower learning rate')\n",
    "elif metrics['mAP50'] < 0.7:\n",
    "    print('     ‚ö†Ô∏è mAP is decent. To improve:')\n",
    "    print('        1. Add 300-500 more images per weak class')\n",
    "    print('        2. Fine-tune: lower lr to 0.0005, train 100 more epochs from best.pt')\n",
    "    print('        3. Try Test-Time Augmentation (TTA) for better inference')\n",
    "else:\n",
    "    print('     ‚úÖ mAP is good! Ready for integration.')\n",
    "    print('        1. Export to ONNX/TensorRT for faster inference')\n",
    "    print('        2. Build the FastAPI backend to serve the model')\n",
    "    print('        3. Connect to RTSP cameras for live detection')\n",
    "\n",
    "print('\\n' + '=' * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46513a84",
   "metadata": {},
   "source": [
    "---\n",
    "## üîÑ Bonus: Resume Training / Fine-Tune\n",
    "\n",
    "If you want to **continue training** from the saved model (e.g., after adding more data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a5d7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title [OPTIONAL] Resume training from last checkpoint\n",
    "# Uncomment and run if you want to resume/fine-tune\n",
    "\n",
    "# from ultralytics import YOLO\n",
    "\n",
    "# # Option A: Resume from where training stopped\n",
    "# model = YOLO(f'{DRIVE_MODELS}/{model_name}/last.pt')\n",
    "# model.train(resume=True)\n",
    "\n",
    "# # Option B: Fine-tune best model on new/updated data\n",
    "# model = YOLO(f'{DRIVE_MODELS}/{model_name}/best.pt')\n",
    "# model.train(\n",
    "#     data=DATA_YAML,\n",
    "#     epochs=100,\n",
    "#     lr0=0.0005,       # Lower LR for fine-tuning\n",
    "#     patience=30,\n",
    "#     freeze=10,        # Freeze first 10 layers (keep learned features)\n",
    "#     project='/content/runs',\n",
    "#     name='smart_lens_v2_finetune',\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
